{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project is creating a Data Lake ETL pipeline to process, clean, and store data related to US I94 immigration data. The data can be used to analyze the immigration flow in the USA.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import configparser\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['aws_key']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['aws_secret']\n",
    "\n",
    "output_data = config['AWS']['out_data']\n",
    "\n",
    "input_I94 = config['INPUT_DATA']['i94_data']\n",
    "input_airport = config['INPUT_DATA']['airport_data']\n",
    "input_country = config['INPUT_DATA']['country_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The Scope of this project is create a Datalake solution, with a ETL pipeline to process, clean and store data related with the I94-immigration data, world weather data and airport codes.\n",
    "\n",
    "The purpose of the pipeline is process and store the data in a parquet file and a star schema\n",
    "\n",
    "Tools: \n",
    "* pandas\n",
    "* python\n",
    "* pyspark to handle the big amount of data\n",
    "* AWS S3 to store the files\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The I94 Immigration dataset must be bought. You can order it with this order form: https://travel.trade.gov/research/cat/doc_order_form.pdf \n",
    "\n",
    "##### The I94 immigration data set comes from https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "* It contains immigration data for the year 2016 from jan to dez.\n",
    "* This dataset has about 3 million rows per file\n",
    "* 28 columns (arrive date, visatypes, airport, gender ...)\n",
    "\n",
    "##### The I94_country data set comes also from https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "* It contains the country codes from the I94 file\n",
    "* This dataset has 289 rows\n",
    "* 2 Columns (I94_code and I94_country)\n",
    "  * This data I copied manually from the I94 description file, which comes with the I94 dataset\n",
    "  \n",
    "##### The I94_port dataset comes also from \n",
    "* It contains the arrive ports from the I94 file\n",
    "* This dataset has rows\n",
    "* 3 Columns (port_code, city, state\\country)\n",
    "  * This data I copied manually from the I94 description file, which comes with the I94 dataset\n",
    "\n",
    "##### The Airport dataset comes from https://datahub.io/core/airport-codes#data\n",
    "* It contains airport data from around the world\n",
    "* This dataset has about 55 tsd rows.\n",
    "* 12 Columns (airporttype, Name, country, Region ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read in the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build or create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the I94 data\n",
    "df_i94 = spark.read.format('csv').option('header',True).load(\"immigration_data_sample.csv\")\n",
    "df_i94.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark.read.format('csv').option('header',True).load('airport-codes_csv.csv')\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- alpha_2: string (nullable = true)\n",
      " |-- alpha_3: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- numeric: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_spark = spark.read.json(\"country_code.json\",multiLine=True)\n",
    "country_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "##### Data quality issues:\n",
    " 1. I94 Data\n",
    "  * Missing data (null or NaN)\n",
    "  * DataType errors (solved with new dataschema)\n",
    " 2. Airport data\n",
    "  * no action needed cames clean\n",
    " 3. Country_code\n",
    "  * no action needed cames clean\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. I94 Data\n",
    " * Replace all null or NaN data with 0\n",
    "2. Airport Data\n",
    " * No cleaning needed\n",
    "3. Country_code\n",
    "  * no action needed cames clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean datasets and schow the first two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|_c0    |cicid    |i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum       |fltno|visatype|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|2027561|4084316.0|2016.0|4.0   |209.0 |209.0 |HHW    |20566.0|1.0    |HI     |20573.0|61.0  |2.0    |1.0  |20160422|N/D     |N/D  |G      |O      |N/D    |M      |1955.0 |07202016|F     |0     |JL     |56582674633.0|00782|WT      |\n",
      "|2171295|4422636.0|2016.0|4.0   |582.0 |582.0 |MCA    |20567.0|1.0    |TX     |20568.0|26.0  |2.0    |1.0  |20160423|MTR     |N/D  |G      |R      |N/D    |M      |1990.0 |10222016|M     |0     |*GA    |94361995930.0|XBLNG|B2      |\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN with 0 for number and N/D for string \n",
    "df_i94_clean = df_i94.na.fill({'i94cit':'0', 'i94res':'0', 'i94port':'N/D', 'arrdate':'0', 'i94mode':'9', 'i94addr':'N/D', 'depdate':'0', 'i94bir':'0',\\\n",
    "                'i94visa':'0', 'count':'0', 'dtadfile':'N/D', 'visapost':'N/D', 'occup':'N/D', 'entdepa':'N/D', 'entdepd':'N/D',\\\n",
    "                'entdepu':'N/D', 'matflag':'N/D', 'biryear':'0', 'dtaddto':'0', 'gender':'N/D', 'insnum':'0', 'airline':'N/D',\\\n",
    "                'admnum':'0', 'fltno':'N/D', 'visatype':'N/D'})\n",
    "df_i94_clean.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "|ident|type         |name                              |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                            |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "|00A  |heliport     |Total Rf Heliport                 |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125     |\n",
      "|00AA |small_airport|Aero B Ranch Airport              |3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022                 |\n",
      "|00AK |small_airport|Lowell Field                      |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968            |\n",
      "|00AL |small_airport|Epps Airpark                      |820         |NA       |US         |US-AL     |Harvest     |00AL    |null     |00AL      |-86.77030181884766, 34.86479949951172  |\n",
      "|00AR |closed       |Newport Hospital & Clinic Heliport|237         |NA       |US         |US-AR     |Newport     |null    |null     |null      |-91.254898, 35.6087                    |\n",
      "|00AS |small_airport|Fulton Airport                    |1100        |NA       |US         |US-OK     |Alex        |00AS    |null     |00AS      |-97.8180194, 34.9428028                |\n",
      "|00AZ |small_airport|Cordes Airport                    |3810        |NA       |US         |US-AZ     |Cordes      |00AZ    |null     |00AZ      |-112.16500091552734, 34.305599212646484|\n",
      "|00CA |small_airport|Goldstone /Gts/ Airport           |3038        |NA       |US         |US-CA     |Barstow     |00CA    |null     |00CA      |-116.888000488, 35.350498199499995     |\n",
      "|00CL |small_airport|Williams Ag Airport               |87          |NA       |US         |US-CA     |Biggs       |00CL    |null     |00CL      |-121.763427, 39.427188                 |\n",
      "|00CN |heliport     |Kitchen Creek Helibase Heliport   |3350        |NA       |US         |US-CA     |Pine Valley |00CN    |null     |00CN      |-116.4597417, 32.7273736               |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean no action needed\n",
    "df_airport.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+-------+\n",
      "|alpha_2|alpha_3|country_name|numeric|\n",
      "+-------+-------+------------+-------+\n",
      "|AF     |AFG    |Afghanistan |004    |\n",
      "|AL     |ALB    |Albania     |008    |\n",
      "+-------+-------+------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_spark.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original column\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (date):\n",
    "    arr_float = float(date)\n",
    "    return (datetime(1960,1,1) + timedelta(days=int(arr_float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94_clean = df_i94_clean.withColumn('arr_date', get_timestamp(df_i94_clean.arrdate))\n",
    "df_i94_clean = df_i94_clean.withColumn('dep_date', get_timestamp(df_i94_clean.depdate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write stagging tables to AWS S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write all datasets to .parquet files to my AWS S3 bucket\n",
    "df_i94_clean.write.save(output_data + \"stagging_i94.parquet\", format= \"parquet\", mode= \"overwrite\")\n",
    "df_airport.write.save(output_data + \"stagging_airport.parquet\", format= \"parquet\", mode= \"overwrite\")\n",
    "country_spark.write.save(output_data + \"stagging_country.parquet\", format=\"parquet\", mode= \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I94 Immigration data models is a star models of four Dimension-tables and one Fact-table:\n",
    "\n",
    "* Dimensions tables:\n",
    " * Admission table\n",
    " * Country table\n",
    " * Airports table\n",
    " * Time table\n",
    "* Fact table:\n",
    " * Immigration table\n",
    "![Stagging Tables](Stagging_tables.png),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![Table Schema](I94_schema.png)\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Set the input, output_data path and the AWS data in the `dl.cfg`.\n",
    "2. The script take the raw data into spark dataframe clean it and write to parquet stagging files\n",
    "3. Read in the stagging files with spark and extract the needed columns for the analytic tables\n",
    "4. Write it table files back as parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in the clean datasets\n",
    "df_i94 = spark.read.format('parquet').load(output_data + \"stagging_i94.parquet\")\n",
    "df_airport = spark.read.format('parquet').load(output_data + \"stagging_airport.parquet\")\n",
    "df_country = spark.read.format('parquet').load(output_data + \"stagging_country.parquet\")\n",
    "\n",
    "# add arr_date to the i94_stagging table\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df_i94.select(\"arr_date\",\n",
    "                          dayofmonth('arr_date').alias('day'),\n",
    "                          weekofyear(\"arr_date\").alias(\"week\"),\n",
    "                          date_format(\"arr_date\", \"u\").alias(\"weekday\"),\n",
    "                          month(\"arr_date\").alias(\"month\"),\n",
    "                          year(\"arr_date\").alias(\"year\")\n",
    "                          )\n",
    "\n",
    "time_table= time_table.write.partitionBy(\"year\",\"month\").format('parquet').save(output_data+\"time_table.parquet\", mode = 'overwrite')\n",
    "\n",
    "# extract columns for the airport table\n",
    "airport_table = df_airport.select(['type','name','iso_country','municipality', 'iata_code'])\n",
    "\n",
    "# write airport table to parquet file\n",
    "airport_table.write.format('parquet').save(output_data + 'airport_table.parquet', mode= 'overwrite')\n",
    "\n",
    "# extract columns for admission table and rename some of them\n",
    "admission_table = df_i94.select(['admnum','cicid', 'dep_date', 'gender', 'biryear', 'i94mode', 'i94visa','visatype', 'visapost'])\\\n",
    "                    .withColumnRenamed('dep_date','departure_date')\\\n",
    "                    .withColumnRenamed('biryear', 'birth_year')\\\n",
    "                    .withColumnRenamed('i94mode', 'mode')\\\n",
    "                    .withColumnRenamed('i94visa', 'visa_code')\n",
    "\n",
    "# write admission table to parquet file\n",
    "admission_table.write.format(\"parquet\").save(output_data + 'admission_table.parquet', mode= 'overwrite')\n",
    "\n",
    "# extract columns for country table and rename some of them\n",
    "country_table = df_country.select(['country_name','alpha_2','alpha_3','numeric'])\\\n",
    "                    .withColumnRenamed('alpha_2', 'two_letter_code')\\\n",
    "                    .withColumnRenamed('alpha_3', 'three_letter_code')\\\n",
    "                    .withColumnRenamed('numeric', 'three_digit_code')\n",
    "\n",
    "# write counrty table to parquet file\n",
    "country_table.write.format('parquet').save(output_data + 'country_table.parquet', mode='overwrite')\n",
    "\n",
    "# extract columns for immigration table and rename some of them\n",
    "immigration_table = df_i94.select(\n",
    "         monotonically_increasing_id().alias('id'),\n",
    "         'arr_date',\\\n",
    "         'dep_date',\\\n",
    "         'i94yr', \\\n",
    "         'i94mon',\\\n",
    "         'i94port',\\\n",
    "         'i94cit',\\\n",
    "         'i94mode',\\\n",
    "         'admnum',\\\n",
    "         'i94visa',\\\n",
    "         'i94bir')\\\n",
    "         .withColumnRenamed('arr_date','arrival_date')\\\n",
    "         .withColumnRenamed('dep_date','departure_date')\\\n",
    "         .withColumnRenamed('i94yr', 'year')\\\n",
    "         .withColumnRenamed('i94mon', 'month')\\\n",
    "         .withColumnRenamed('i94port', 'airport')\\\n",
    "         .withColumnRenamed('i94cit', 'origin_country')\\\n",
    "         .withColumnRenamed('i94mode', 'mode')\\\n",
    "         .withColumnRenamed('i94visa', 'visa_code')\\\n",
    "         .withColumnRenamed('i94bir', 'age')\n",
    "\n",
    "# write immigration table to parquet file partitioned by year and month\n",
    "immigration_table.write.partitionBy('year','month').format('parquet')\\\n",
    "    .save(output_data + 'immigration_table.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-------+----+-----+\n",
      "|           arr_date|day|week|weekday|year|month|\n",
      "+-------------------+---+----+-------+----+-----+\n",
      "|2016-04-22 00:00:00| 22|  16|      5|2016|    4|\n",
      "|2016-04-23 00:00:00| 23|  16|      6|2016|    4|\n",
      "+-------------------+---+----+-------+----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in time_table for quality check\n",
    "time_table = spark.read.format('parquet').load(output_data + \"time_table.parquet\")\n",
    "time_table.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality Check for admission table rows > 0\n",
    "admission_table.createOrReplaceTempView(\"admission_table\")\n",
    "admission_table_check = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM admission_table\n",
    "\"\"\")\n",
    "admission_table_check.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "admission = spark.sql(\"\"\"\n",
    "    SELECT COUNT (*)\n",
    "    FROM admission_table\n",
    "    WHERE admnum == \"\"\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "admission.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality Check for country table rows > 0\n",
    "country_table.createOrReplaceTempView(\"country_table\")\n",
    "country_table_check = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM country_table\n",
    "        \"\"\")\n",
    "country_table_check.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----------------+----------------+\n",
      "|        country_name|two_letter_code|three_letter_code|three_digit_code|\n",
      "+--------------------+---------------+-----------------+----------------+\n",
      "|United States of ...|             US|              USA|             840|\n",
      "+--------------------+---------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_table = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM country_table\n",
    "    \"\"\")\n",
    "country_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   55075|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality Check for airport table rows > 0\n",
    "airport_table.createOrReplaceTempView(\"airport_table\")\n",
    "airport_table_check = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM airport_table\n",
    "        \"\"\")\n",
    "airport_table_check.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   22757|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air_table = spark.sql(\"\"\"\n",
    "    SELECT COUNT (*)\n",
    "    FROM airport_table\n",
    "    WHERE iso_country == 'US'\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "air_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality Check for time table rows > 0\n",
    "time_table.createOrReplaceTempView(\"time_table\")\n",
    "time_table_check = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM time_table\n",
    "        \"\"\")\n",
    "time_table_check.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      37|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time = spark.sql(\"\"\"\n",
    "    SELECT COUNT (*)\n",
    "    FROM time_table\n",
    "    WHERE day == '23'\n",
    "\"\"\")\n",
    "time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quality Check for immigration table rows > 0\n",
    "immigration_table.createOrReplaceTempView(\"immigration_table\")\n",
    "immigration_table_check = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM immigration_table\n",
    "        \"\"\")\n",
    "immigration_table_check.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      23|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_table = spark.sql(\"\"\"\n",
    "    SELECT COUNT (*)\n",
    "    FROM immigration_table\n",
    "    WHERE visa_code == '2.0' AND mode == '3.0'\n",
    "    \"\"\")\n",
    "immi_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The Data dictionary you can find as a excel file named Data Dictionary.xlsx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Why I choice this Tools?\n",
    "* Pandas: is one of the most used data manipulation library build on python\n",
    "* Python: is one of the most comun programming language for data analysis\n",
    "* Pyspark(Spark): spark can handle fast a large amount of data(Big Data)\n",
    "* AWS S3: on AWS you can store relative cheap a large amount of data in a Cloud\n",
    "\n",
    "### How often you should update the dataset?\n",
    "* These data are published monthly, so they should be updated monthly\n",
    "\n",
    "### What shoud I do with the following problems?\n",
    "* The dataset was increased by 100x\n",
    " * In this case you should change the storage option on AWS (buy more space and power(cpu´s))\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * In this case you can implement a DAG that can update the data on a daily base\n",
    "* The database needed to be accessed by 100+ people.\n",
    " * If the date already stored in in a Cloud there should be no problem but you can create also new clusters to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
